Maximum Likelihood Estimation & EM Algorithm â€“ Assignment 3
!Python
!Status
!License
ğŸ“‹ Project Overview
This project implements Maximum Likelihood Estimation (MLE) and the Expectation-Maximization (EM) algorithm to analyze Gaussian data with missing values. It compares different parameter estimation strategies and highlights how missing data affects the accuracy of statistical estimates.
ğŸ¯ Goals

âœ… Implement MLE for Gaussian distributions (1D, 2D, 3D)
âœ… Develop EM algorithm to handle missing data
âœ… Compare results between complete and incomplete datasets
âœ… Visualize clusters and algorithm convergence
âœ… Analyze the impact of different initialization strategies

ğŸ“Š Datasets
Category Ï‰â‚ (Omega 1)

10 three-dimensional points with features [xâ‚, xâ‚‚, xâ‚ƒ]
Missing data: xâ‚ƒ is missing in even-indexed points (2, 4, 6, 8, 10)
Missing rate: 50% in the xâ‚ƒ dimension

Category Ï‰â‚‚ (Omega 2)

10 complete three-dimensional points
Used for: Separable model with diagonal covariance matrix

ğŸš€ Key Features
1. Traditional MLE (mle_omega1.py)

Univariate estimation for each feature
Bivariate analysis for feature pairs
Full 3D analysis
Comparison of mean and variance estimates

2. EM Algorithm (em_algorithm.py / em_algorithm_ascii.py)

Full EM implementation for handling missing data
Two initialization strategies:

Zero initialization: xâ‚ƒ = 0
Mean-based initialization: xâ‚ƒ = (xâ‚ + xâ‚‚)/2


Comparison with complete data (ground truth)
Detailed convergence analysis

3. Advanced Visualizations

3D cluster plots
Convergence tracking
Side-by-side comparisons
Correlation matrices
Feature-wise error analysis

ğŸ“ Project Structure
assignment3/
â”œâ”€â”€ ğŸ”§ Core Code
â”‚   â”œâ”€â”€ mle_omega1.py              # Traditional MLE (complete data)
â”‚   â”œâ”€â”€ em_algorithm.py            # EM algorithm (Unicode version)
â”‚   â””â”€â”€ em_algorithm_ascii.py      # EM algorithm (ASCII version)
â”‚
â”œâ”€â”€ ğŸ“Š Visualization & Analysis
â”‚   â”œâ”€â”€ cluster_visualization.py   # Comprehensive visualizations
â”‚   â”œâ”€â”€ results_summary.py         # Summary of results
â”‚   â””â”€â”€ show_plots.py              # Plot viewer
â”‚
â”œâ”€â”€ ğŸ“ˆ Results (Graphs)
â”‚   â”œâ”€â”€ em_convergence_analysis.png
â”‚   â”œâ”€â”€ em_complete_comparison.png
â”‚   â””â”€â”€ cluster_analysis_comprehensive.png
â”‚
â”œâ”€â”€ ğŸ“š Documentation
â”‚   â”œâ”€â”€ README.md                  # This file
â”‚   â”œâ”€â”€ complete_vs_missing_comparison.md
â”‚   â”œâ”€â”€ em_summary_report.md
â”‚   â””â”€â”€ requirements.txt           # Dependencies
â”‚
â””â”€â”€ ğŸ“ Misc
    â”œâ”€â”€ em_output.txt              # Execution logs
    â””â”€â”€ __pycache__/               # Python cache

ğŸ› ï¸ Installation & Usage
Prerequisites
ShellPython 3.13+pip (Python package manager)Mostrar mais linhas
1. Clone the repository
Shellgit clone https://github.com/your-username/mle-em-algorithm-assignment3.gitcd mle-em-algorithm-assignment3Mostrar mais linhas
2. Install dependencies
Shellpip install -r requirements.txt``Mostrar mais linhas
3. Run the algorithms
Traditional MLE (complete data):
Shellpython mle_omega1.pyMostrar mais linhas
EM Algorithm (with missing data):
Shellpython em_algorithm.py# or ASCII-compatible version:python em_algorithm_ascii.pyMostrar mais linhas
View results:
Shellpython show_plots.pypython results_summary.pyMostrar mais linhas
ğŸ“Š Key Findings
âœ… EM Algorithm Successes

Perfect recovery of observed dimensions (xâ‚, xâ‚‚)
Zero error in Î¼â‚, Î¼â‚‚, Ïƒâ‚Â², Ïƒâ‚‚Â²
Robust convergence in 16 iterations
Initialization-independent results

âŒ Identified Limitations

Systematic bias in the missing dimension (xâ‚ƒ)
1.684 units error in Î¼â‚ƒ (185% relative error)
61% underestimation in Ïƒâ‚ƒÂ²
Cluster compression (54% volume reduction)

ğŸ¯ Key Insights

MLE preserves information in observed dimensions
Missing data patterns significantly affect estimates
EM is robust, but introduces predictable bias
Cluster structure is partially recoverable

ğŸ“ˆ Generated Visuals
1. Convergence Analysis

Log-likelihood evolution
Initialization strategy comparison
Identical convergence behavior

2. Complete vs Missing Comparison

Side-by-side parameter estimates
Feature-wise error analysis
Visual impact of missing data

3. Comprehensive Cluster Analysis

3D data visualization
2D projections
Correlation matrices
Detailed error breakdown

ğŸ”¬ Theoretical Foundations
Maximum Likelihood Estimation (MLE)
Plain Textmath nÃ£o tem suporte total. O realce de sintaxe Ã© baseado em Plain Text.Î¼Ì‚ = (1/n) Ã— Î£áµ¢ xáµ¢Î£Ì‚ = (1/n) Ã— Î£áµ¢ (xáµ¢ - Î¼Ì‚)(xáµ¢ - Î¼Ì‚)áµ€``Mostrar mais linhas
Expectation-Maximization (EM) Algorithm

E-step: E[Xâ‚ƒ|Xâ‚,Xâ‚‚] = Î¼â‚ƒ + Î£â‚ƒâ‚Î£â‚â‚â»Â¹(Xâ‚â‚‚ - Î¼â‚â‚‚)
M-step: Update parameters using "completed" data
Convergence: Based on log-likelihood improvement

ğŸ™ Acknowledgments

Based on concepts from Machine Learning and Statistics
Algorithms grounded in Maximum Likelihood Estimation theory
Visualizations inspired by Data Science best practices
