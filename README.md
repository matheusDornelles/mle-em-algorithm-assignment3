# Maximum Likelihood Estimation & EM Algorithm - Assignment 3

![Python](https://img.shields.io/badge/Python-3.13-blue.svg)
![Status](https://img.shields.io/badge/Status-Complete-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

## ğŸ“‹ DescriÃ§Ã£o do Projeto

Este projeto implementa algoritmos de **EstimaÃ§Ã£o de MÃ¡xima VerossimilhanÃ§a (MLE)** e **Expectation-Maximization (EM)** para anÃ¡lise de dados Gaussianos com dados faltantes. O trabalho compara diferentes abordagens de estimaÃ§Ã£o de parÃ¢metros e demonstra o impacto de dados missing na qualidade das estimativas.

## ğŸ¯ Objetivos

- âœ… Implementar MLE para distribuiÃ§Ãµes Gaussianas (1D, 2D, 3D)
- âœ… Desenvolver algoritmo EM para dados com valores faltantes
- âœ… Comparar resultados entre dados completos e incompletos
- âœ… Visualizar clusters e convergÃªncia dos algoritmos
- âœ… Analisar impacto de diferentes estratÃ©gias de inicializaÃ§Ã£o

## ğŸ“Š Datasets

### Categoria Ï‰â‚ (Omega 1)
- **10 pontos tridimensionais** com caracterÃ­sticas [xâ‚, xâ‚‚, xâ‚ƒ]
- **Dados faltantes**: xâ‚ƒ missing nos pontos pares (2, 4, 6, 8, 10)
- **Taxa de missing**: 50% na dimensÃ£o xâ‚ƒ

### Categoria Ï‰â‚‚ (Omega 2)  
- **10 pontos tridimensionais** completos
- **Usado para**: Modelo separÃ¡vel com matriz de covariÃ¢ncia diagonal

## ğŸš€ Principais Funcionalidades

### 1. **MLE Tradicional** (`mle_omega1.py`)
- EstimaÃ§Ã£o univariada para cada caracterÃ­stica
- AnÃ¡lise bivariada para pares de caracterÃ­sticas
- AnÃ¡lise trivariada completa (3D)
- ComparaÃ§Ã£o de estimativas de mÃ©dia e variÃ¢ncia

### 2. **Algoritmo EM** (`em_algorithm.py` / `em_algorithm_ascii.py`)
- ImplementaÃ§Ã£o completa do EM para dados faltantes
- Duas estratÃ©gias de inicializaÃ§Ã£o:
  - InicializaÃ§Ã£o zero: xâ‚ƒ = 0
  - InicializaÃ§Ã£o mÃ©dia: xâ‚ƒ = (xâ‚ + xâ‚‚)/2
- ComparaÃ§Ã£o com dados completos (ground truth)
- AnÃ¡lise de convergÃªncia detalhada

### 3. **VisualizaÃ§Ãµes AvanÃ§adas**
- GrÃ¡ficos 3D dos clusters
- AnÃ¡lise de convergÃªncia
- ComparaÃ§Ãµes side-by-side
- Matrizes de correlaÃ§Ã£o
- AnÃ¡lise de erros por caracterÃ­stica

## ğŸ“ Estrutura do Projeto

```
assignment3/
â”œâ”€â”€ ğŸ”§ CÃ³digo Principal
â”‚   â”œâ”€â”€ mle_omega1.py              # MLE tradicional (dados completos)
â”‚   â”œâ”€â”€ em_algorithm.py            # Algoritmo EM (versÃ£o Unicode)
â”‚   â””â”€â”€ em_algorithm_ascii.py      # Algoritmo EM (versÃ£o ASCII)
â”‚
â”œâ”€â”€ ğŸ“Š VisualizaÃ§Ã£o e AnÃ¡lise
â”‚   â”œâ”€â”€ cluster_visualization.py   # VisualizaÃ§Ãµes abrangentes
â”‚   â”œâ”€â”€ results_summary.py         # Resumo visual dos resultados
â”‚   â””â”€â”€ show_plots.py             # Visualizador de grÃ¡ficos
â”‚
â”œâ”€â”€ ğŸ“ˆ Resultados (GrÃ¡ficos)
â”‚   â”œâ”€â”€ em_convergence_analysis.png
â”‚   â”œâ”€â”€ em_complete_comparison.png
â”‚   â””â”€â”€ cluster_analysis_comprehensive.png
â”‚
â”œâ”€â”€ ğŸ“š DocumentaÃ§Ã£o
â”‚   â”œâ”€â”€ README.md                  # Este arquivo
â”‚   â”œâ”€â”€ complete_vs_missing_comparison.md
â”‚   â”œâ”€â”€ em_summary_report.md
â”‚   â””â”€â”€ requirements.txt           # DependÃªncias
â”‚
â””â”€â”€ ğŸ“ Outros
    â”œâ”€â”€ em_output.txt             # Logs de execuÃ§Ã£o
    â””â”€â”€ __pycache__/              # Cache Python
```

## ğŸ› ï¸ InstalaÃ§Ã£o e Uso

### PrÃ©-requisitos
```bash
Python 3.13+
pip (gerenciador de pacotes Python)
```

### 1. Clone o repositÃ³rio
```bash
git clone https://github.com/seu-usuario/mle-em-algorithm-assignment3.git
cd mle-em-algorithm-assignment3
```

### 2. Instale as dependÃªncias
```bash
pip install -r requirements.txt
```

### 3. Execute os algoritmos

#### MLE Tradicional (dados completos):
```bash
python mle_omega1.py
```

#### Algoritmo EM (dados faltantes):
```bash
python em_algorithm.py
# ou versÃ£o ASCII compatÃ­vel:
python em_algorithm_ascii.py
```

#### Visualizar resultados:
```bash
python show_plots.py
python results_summary.py
```

## ğŸ“Š Principais Resultados

### âœ… **Sucessos do Algoritmo EM**
- **RecuperaÃ§Ã£o perfeita** das dimensÃµes observadas (xâ‚, xâ‚‚)
- **Erro zero** em Î¼â‚, Î¼â‚‚, Ïƒâ‚Â², Ïƒâ‚‚Â²
- **ConvergÃªncia robusta** em 16 iteraÃ§Ãµes
- **IndependÃªncia da inicializaÃ§Ã£o**

### âŒ **LimitaÃ§Ãµes Identificadas**  
- **ViÃ©s sistemÃ¡tico** na dimensÃ£o faltante (xâ‚ƒ)
- **Erro de 1.684 unidades** em Î¼â‚ƒ (185% erro relativo)
- **SubestimaÃ§Ã£o de 61%** em Ïƒâ‚ƒÂ²
- **CompressÃ£o do cluster** (54% reduÃ§Ã£o no volume)

### ğŸ¯ **Insights Principais**
1. **MLE preserva informaÃ§Ã£o** nas dimensÃµes observadas perfeitamente
2. **PadrÃ£o de missing data** afeta significativamente estimativas
3. **EM Ã© robusto** mas introduz viÃ©s previsÃ­vel
4. **Estrutura de cluster** Ã© parcialmente recuperÃ¡vel

## ğŸ“ˆ VisualizaÃ§Ãµes Geradas

### 1. **AnÃ¡lise de ConvergÃªncia**
- EvoluÃ§Ã£o da log-likelihood
- ComparaÃ§Ã£o entre estratÃ©gias de inicializaÃ§Ã£o
- DemonstraÃ§Ã£o de convergÃªncia idÃªntica

### 2. **ComparaÃ§Ã£o Completo vs Missing**
- Side-by-side dos parÃ¢metros estimados
- AnÃ¡lise de erros por caracterÃ­stica
- Impacto visual dos dados faltantes

### 3. **AnÃ¡lise Abrangente de Clusters**
- VisualizaÃ§Ã£o 3D dos dados
- ProjeÃ§Ãµes 2D
- Matrizes de correlaÃ§Ã£o
- AnÃ¡lise detalhada de erros

## ğŸ”¬ Fundamentos TeÃ³ricos

### **MLE (Maximum Likelihood Estimation)**
```
Î¼Ì‚ = (1/n) Ã— Î£áµ¢ xáµ¢
Î£Ì‚ = (1/n) Ã— Î£áµ¢ (xáµ¢ - Î¼Ì‚)(xáµ¢ - Î¼Ì‚)áµ€
```

### **Algoritmo EM**
- **E-step**: E[Xâ‚ƒ|Xâ‚,Xâ‚‚] = Î¼â‚ƒ + Î£â‚ƒâ‚Î£â‚â‚â»Â¹(Xâ‚â‚‚ - Î¼â‚â‚‚)
- **M-step**: AtualizaÃ§Ã£o dos parÃ¢metros com dados "completos"
- **ConvergÃªncia**: Baseada na log-likelihood

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor:

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ‘¥ Autores

- **Seu Nome** - *Trabalho inicial* - [SeuGitHub](https://github.com/seu-usuario)

## ğŸ™ Agradecimentos

- ImplementaÃ§Ã£o baseada em conceitos de Machine Learning e EstatÃ­stica
- Algoritmos fundamentados em teoria de EstimaÃ§Ã£o de MÃ¡xima VerossimilhanÃ§a
- VisualizaÃ§Ãµes inspiradas em prÃ¡ticas de Data Science

## ğŸ“š ReferÃªncias

1. Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm.
2. Bishop, C. M. (2006). Pattern Recognition and Machine Learning.
3. Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective.

---

â­ **Se este projeto foi Ãºtil para vocÃª, considere dar uma estrela!** â­